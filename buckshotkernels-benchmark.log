buckshotkernels.py
ğŸ” CUDA Toolkit: Found at /usr
ğŸš€ TERNARYCORE CUSTOM KERNEL DEMONSTRATION
============================================================
ğŸ’¥ Building kernels from scratch - no libraries, pure metal!

ğŸ”§ Compiling custom kernels...
ğŸš€ Initializing TernaryKernelManager...
ğŸ”¨ Compiling CPU kernels...
ğŸ”§ CPU compiler: gcc
ğŸš€ SIMD features: ['SSE2', 'AVX', 'AVX2']
ğŸ“ Temp directory: /tmp/ternary_cpu_kernels_rvruubor
âœ… CPU kernel source written: /tmp/ternary_cpu_kernels_rvruubor/ternary_cpu_kernels.c
ğŸ”¨ Compiling CPU kernels...
Command: gcc -O3 -ffast-math -shared -fPIC -march=native -mavx2 /tmp/ternary_cpu_kernels_rvruubor/ternary_cpu_kernels.c -o /tmp/ternary_cpu_kernels_rvruubor/ternary_cpu_kernels.so
âœ… CPU kernels compiled successfully!
ğŸ“¦ Library: /tmp/ternary_cpu_kernels_rvruubor/ternary_cpu_kernels.so
âœ… CPU kernels loaded successfully!
âœ… CPU kernels ready!
ğŸ”¨ Compiling CUDA kernels...
ğŸ”§ CUDA compiler ready: /usr/bin/nvcc
ğŸ“ Temp directory: /tmp/ternary_kernels_pjc0a4ix
ğŸ¯ Target GPU architecture: sm_61
âœ… CUDA kernel source written: /tmp/ternary_kernels_pjc0a4ix/ternary_kernels.cu
ğŸ”¨ Compiling CUDA kernels...
Command: /usr/bin/nvcc -shared -Xcompiler -fPIC -arch sm_61 --ptxas-options=-v -O3 --use_fast_math -DCUDA_KERNEL_COMPILATION /tmp/ternary_kernels_pjc0a4ix/ternary_kernels.cu -o /tmp/ternary_kernels_pjc0a4ix/ternary_kernels.so
âœ… CUDA kernels compiled successfully!
ğŸ“¦ Library: /tmp/ternary_kernels_pjc0a4ix/ternary_kernels.so
Compiler output:
ptxas info    : 0 bytes gmem
ptxas info    : Compiling entry function 'ternary_reduce_sum_kernel' for 'sm_61'
ptxas info    : Function properties for ternary_reduce_sum_kernel
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 8 registers, 344 bytes cmem[0]
ptxas info    : Compiling entry function 'ternary_quantize_kernel' for 'sm_61'
ptxas info    : Function properties for ternary_quantize_kernel
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 6 registers, 344 bytes cmem[0]
ptxas info    : Compiling entry function 'ternary_conv2d_kernel' for 'sm_61'
ptxas info    : Function properties for ternary_conv2d_kernel
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 31 registers, 396 bytes cmem[0]
ptxas info    : Compiling entry function 'ternary_matmul_kernel' for 'sm_61'
ptxas info    : Function properties for ternary_matmul_kernel
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 25 registers, 512 bytes smem, 356 bytes cmem[0]

âœ… CUDA kernels loaded successfully!
âœ… CUDA kernels ready!

ğŸ§ª Testing custom kernels...
ğŸ¯ Test matrices: (512, 512) @ (512, 512)
   Sparsity A: 49.9%
   Sparsity B: 49.8%

âš¡ Testing ternary matrix multiplication...
ğŸ–¥ CPU kernel: 512x512 @ 512x512 in 37.14ms
âŒ Results differ from NumPy
   Max difference: 8926

âš¡ Testing ternary quantization...
âš¡ Quantized 1,000,000 elements in 0.37ms
âœ… Quantized values: [-1  0  1]
   Elements processed: 1,000,000
   Throughput: 1694.7 Melem/s

ğŸ¯ Testing OptimizedTernaryTensor...
ğŸ”§ Initializing kernel manager...
ğŸš€ Initializing TernaryKernelManager...
ğŸ”¨ Compiling CPU kernels...
ğŸ”§ CPU compiler: gcc
ğŸš€ SIMD features: ['SSE2', 'AVX', 'AVX2']
ğŸ“ Temp directory: /tmp/ternary_cpu_kernels_7410krw_
âœ… CPU kernel source written: /tmp/ternary_cpu_kernels_7410krw_/ternary_cpu_kernels.c
ğŸ”¨ Compiling CPU kernels...
Command: gcc -O3 -ffast-math -shared -fPIC -march=native -mavx2 /tmp/ternary_cpu_kernels_7410krw_/ternary_cpu_kernels.c -o /tmp/ternary_cpu_kernels_7410krw_/ternary_cpu_kernels.so
âœ… CPU kernels compiled successfully!
ğŸ“¦ Library: /tmp/ternary_cpu_kernels_7410krw_/ternary_cpu_kernels.so
âœ… CPU kernels loaded successfully!
âœ… CPU kernels ready!
ğŸ”¨ Compiling CUDA kernels...
ğŸ”§ CUDA compiler ready: /usr/bin/nvcc
ğŸ“ Temp directory: /tmp/ternary_kernels_i4pcgrvs
ğŸ¯ Target GPU architecture: sm_61
âœ… CUDA kernel source written: /tmp/ternary_kernels_i4pcgrvs/ternary_kernels.cu
ğŸ”¨ Compiling CUDA kernels...
Command: /usr/bin/nvcc -shared -Xcompiler -fPIC -arch sm_61 --ptxas-options=-v -O3 --use_fast_math -DCUDA_KERNEL_COMPILATION /tmp/ternary_kernels_i4pcgrvs/ternary_kernels.cu -o /tmp/ternary_kernels_i4pcgrvs/ternary_kernels.so
âœ… CUDA kernels compiled successfully!
ğŸ“¦ Library: /tmp/ternary_kernels_i4pcgrvs/ternary_kernels.so
Compiler output:
ptxas info    : 0 bytes gmem
ptxas info    : Compiling entry function 'ternary_reduce_sum_kernel' for 'sm_61'
ptxas info    : Function properties for ternary_reduce_sum_kernel
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 8 registers, 344 bytes cmem[0]
ptxas info    : Compiling entry function 'ternary_quantize_kernel' for 'sm_61'
ptxas info    : Function properties for ternary_quantize_kernel
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 6 registers, 344 bytes cmem[0]
ptxas info    : Compiling entry function 'ternary_conv2d_kernel' for 'sm_61'
ptxas info    : Function properties for ternary_conv2d_kernel
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 31 registers, 396 bytes cmem[0]
ptxas info    : Compiling entry function 'ternary_matmul_kernel' for 'sm_61'
ptxas info    : Function properties for ternary_matmul_kernel
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 25 registers, 512 bytes smem, 356 bytes cmem[0]

âœ… CUDA kernels loaded successfully!
âœ… CUDA kernels ready!
ğŸ–¥ CPU kernel: 512x512 @ 512x512 in 27.63ms
âœ… Tensor operation: 28.08ms
   Result shape: (512, 512)
   Result dtype: int8

ğŸ“Š Running comprehensive benchmark...
ğŸƒ BENCHMARKING CUSTOM KERNELS
==================================================

ğŸ¯ Testing 256x256 matrices...
  ğŸ–¥ Testing CPU kernel...
ğŸ–¥ CPU kernel: 256x256 @ 256x256 in 1.89ms
ğŸ–¥ CPU kernel: 256x256 @ 256x256 in 1.89ms
ğŸ–¥ CPU kernel: 256x256 @ 256x256 in 1.62ms
ğŸ–¥ CPU kernel: 256x256 @ 256x256 in 1.71ms
ğŸ–¥ CPU kernel: 256x256 @ 256x256 in 2.25ms
  ğŸš€ Testing CUDA kernel...
ğŸš€ CUDA kernel: 256x256 @ 256x256 in 3477.47ms
ğŸš€ CUDA kernel: 256x256 @ 256x256 in 0.44ms
ğŸš€ CUDA kernel: 256x256 @ 256x256 in 0.33ms
ğŸš€ CUDA kernel: 256x256 @ 256x256 in 0.35ms
ğŸš€ CUDA kernel: 256x256 @ 256x256 in 0.27ms
  âš  CPU and CUDA results differ
  âš¡ Testing quantization kernel...
âš¡ Quantized 65,536 elements in 0.07ms
âš¡ Quantized 65,536 elements in 0.02ms
âš¡ Quantized 65,536 elements in 0.02ms
âš¡ Quantized 65,536 elements in 0.02ms
âš¡ Quantized 65,536 elements in 0.02ms
âš¡ Quantized 65,536 elements in 0.02ms
âš¡ Quantized 65,536 elements in 0.02ms
âš¡ Quantized 65,536 elements in 0.02ms
âš¡ Quantized 65,536 elements in 0.02ms
âš¡ Quantized 65,536 elements in 0.02ms
    CPU: 1.98ms (16.95 GFLOPS)
    CUDA: 845.04ms (0.04 GFLOPS, 0.00x speedup)
    Quantization: 0.06ms (1034.1 Melem/s)

ğŸ¯ Testing 512x512 matrices...
  ğŸ–¥ Testing CPU kernel...
ğŸ–¥ CPU kernel: 512x512 @ 512x512 in 15.44ms
ğŸ–¥ CPU kernel: 512x512 @ 512x512 in 15.26ms
ğŸ–¥ CPU kernel: 512x512 @ 512x512 in 14.18ms
ğŸ–¥ CPU kernel: 512x512 @ 512x512 in 14.51ms
ğŸ–¥ CPU kernel: 512x512 @ 512x512 in 13.55ms
  ğŸš€ Testing CUDA kernel...
ğŸš€ CUDA kernel: 512x512 @ 512x512 in 0.72ms
ğŸš€ CUDA kernel: 512x512 @ 512x512 in 0.37ms
ğŸš€ CUDA kernel: 512x512 @ 512x512 in 0.34ms
ğŸš€ CUDA kernel: 512x512 @ 512x512 in 0.38ms
ğŸš€ CUDA kernel: 512x512 @ 512x512 in 0.35ms
  âš  CPU and CUDA results differ
  âš¡ Testing quantization kernel...
âš¡ Quantized 262,144 elements in 0.06ms
âš¡ Quantized 262,144 elements in 0.05ms
âš¡ Quantized 262,144 elements in 0.05ms
âš¡ Quantized 262,144 elements in 0.05ms
âš¡ Quantized 262,144 elements in 0.05ms
âš¡ Quantized 262,144 elements in 0.05ms
âš¡ Quantized 262,144 elements in 0.05ms
âš¡ Quantized 262,144 elements in 0.05ms
âš¡ Quantized 262,144 elements in 0.05ms
âš¡ Quantized 262,144 elements in 0.05ms
    CPU: 14.84ms (18.09 GFLOPS)
    CUDA: 0.95ms (283.10 GFLOPS, 15.65x speedup)
    Quantization: 0.09ms (2899.2 Melem/s)

ğŸ¯ Testing 1024x1024 matrices...
  ğŸ–¥ Testing CPU kernel...
ğŸ–¥ CPU kernel: 1024x1024 @ 1024x1024 in 123.28ms
ğŸ–¥ CPU kernel: 1024x1024 @ 1024x1024 in 122.08ms
ğŸ–¥ CPU kernel: 1024x1024 @ 1024x1024 in 123.81ms
ğŸ–¥ CPU kernel: 1024x1024 @ 1024x1024 in 119.34ms
ğŸ–¥ CPU kernel: 1024x1024 @ 1024x1024 in 116.31ms
  ğŸš€ Testing CUDA kernel...
ğŸš€ CUDA kernel: 1024x1024 @ 1024x1024 in 1.69ms
ğŸš€ CUDA kernel: 1024x1024 @ 1024x1024 in 0.92ms
ğŸš€ CUDA kernel: 1024x1024 @ 1024x1024 in 0.92ms
ğŸš€ CUDA kernel: 1024x1024 @ 1024x1024 in 0.91ms
ğŸš€ CUDA kernel: 1024x1024 @ 1024x1024 in 0.89ms
  âš  CPU and CUDA results differ
  âš¡ Testing quantization kernel...
âš¡ Quantized 1,048,576 elements in 0.49ms
âš¡ Quantized 1,048,576 elements in 0.38ms
âš¡ Quantized 1,048,576 elements in 0.43ms
âš¡ Quantized 1,048,576 elements in 0.50ms
âš¡ Quantized 1,048,576 elements in 0.51ms
âš¡ Quantized 1,048,576 elements in 0.49ms
âš¡ Quantized 1,048,576 elements in 0.48ms
âš¡ Quantized 1,048,576 elements in 0.49ms
âš¡ Quantized 1,048,576 elements in 0.56ms
âš¡ Quantized 1,048,576 elements in 0.57ms
    CPU: 121.37ms (17.69 GFLOPS)
    CUDA: 3.16ms (678.53 GFLOPS, 38.35x speedup)
    Quantization: 0.68ms (1531.0 Melem/s)

ğŸ¯ Testing 2048x2048 matrices...
  ğŸ–¥ Testing CPU kernel...
ğŸ–¥ CPU kernel: 2048x2048 @ 2048x2048 in 927.01ms
ğŸ–¥ CPU kernel: 2048x2048 @ 2048x2048 in 894.97ms
ğŸ–¥ CPU kernel: 2048x2048 @ 2048x2048 in 994.99ms
ğŸ–¥ CPU kernel: 2048x2048 @ 2048x2048 in 990.68ms
ğŸ–¥ CPU kernel: 2048x2048 @ 2048x2048 in 977.30ms
  ğŸš€ Testing CUDA kernel...
ğŸš€ CUDA kernel: 2048x2048 @ 2048x2048 in 4.76ms
ğŸš€ CUDA kernel: 2048x2048 @ 2048x2048 in 3.84ms
ğŸš€ CUDA kernel: 2048x2048 @ 2048x2048 in 3.86ms
ğŸš€ CUDA kernel: 2048x2048 @ 2048x2048 in 4.14ms
ğŸš€ CUDA kernel: 2048x2048 @ 2048x2048 in 4.19ms
  âš  CPU and CUDA results differ
  âš¡ Testing quantization kernel...
âš¡ Quantized 4,194,304 elements in 1.95ms
âš¡ Quantized 4,194,304 elements in 1.81ms
âš¡ Quantized 4,194,304 elements in 1.88ms
âš¡ Quantized 4,194,304 elements in 1.81ms
âš¡ Quantized 4,194,304 elements in 1.68ms
âš¡ Quantized 4,194,304 elements in 1.60ms
âš¡ Quantized 4,194,304 elements in 1.67ms
âš¡ Quantized 4,194,304 elements in 1.57ms
âš¡ Quantized 4,194,304 elements in 1.57ms
âš¡ Quantized 4,194,304 elements in 1.59ms
    CPU: 957.40ms (17.94 GFLOPS)
    CUDA: 12.00ms (1431.31 GFLOPS, 79.76x speedup)
    Quantization: 2.17ms (1932.1 Melem/s)

ğŸ† CUSTOM KERNEL PERFORMANCE SUMMARY
==================================================

256x256 matrices:
  CPU: 16.95 GFLOPS
  CUDA: 0.04 GFLOPS
  Quantization: 1034.1 Melem/s

512x512 matrices:
  CPU: 18.09 GFLOPS
  CUDA: 283.10 GFLOPS
  Quantization: 2899.2 Melem/s

1024x1024 matrices:
  CPU: 17.69 GFLOPS
  CUDA: 678.53 GFLOPS
  Quantization: 1531.0 Melem/s

2048x2048 matrices:
  CPU: 17.94 GFLOPS
  CUDA: 1431.31 GFLOPS
  Quantization: 1932.1 Melem/s

âœ… CUSTOM KERNEL DEMONSTRATION COMPLETE!
ğŸ’ª Raw performance achieved through custom compilation!

ğŸ‰ KERNEL COMPILATION SUCCESSFUL!
